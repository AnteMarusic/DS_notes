\documentclass[12pt,oneside,a4paper]{book}
\usepackage[margin=0.4in]{geometry} % This sets all margins to 1 inch
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}
\usepackage{enumitem}
\usepackage{verbatim}
\usepackage{multicol}
\pagestyle{empty}
\footnotesize
\setlength{\columnsep}{1cm}
\usepackage{titlesec}

% Custom styles
\titleformat{\section}{\fontsize{13}{12}\bfseries}{}{0pt}{}
\titlespacing{\section}{0pt}{6pt}{2pt}

\titleformat{\subsection}{\fontsize{12}{11}\bfseries\itshape}{}{0pt}{}
\titlespacing{\subsection}{0pt}{4pt}{1pt}


\begin{document}

\begin{multicols}{2}
  \chapter{Intro}
  \section{IP (Internet Protocol)}
  Best effort delivery: The protocol doesn't provide guarantees on delivery or
  QoS. More in detail:
  \begin{itemize}
    \item Unreliable: packet loss, retrasmission, packed delay, unordered arrival.
    \item No QoS: bandwith, latency and jitter vary with network conditions. There are no
          guarantees on their value. No priority amongst users: Everyone experiences the
          same best effort service. No bitrate control to adapt to network conditions
          (reduce bitrate to decrease latency)
  \end{itemize}

  Reliable delivery and QoS can be built on top of best effort delivery.

  \section{TCP (Trasmission Control Protocol) and UDP (User Datagram Protocol)}
  \textbf{TCP}
  \begin{itemize}
    \item Reliable (error checked (only in v4. v6 delegates the control at the frame
          level))
    \item Connection oriented (three way handshake SYN(x) SYN-ACK(x+1, y) ACK(y+1))
    \item Network congestion avoidance
  \end{itemize}
  In case of failure in the communication the source is notified. The protocol is
  implemented with positive ack + retrasmission, so sent packets are kept
  by the source until an ack is received. They are retrasmitted if the ack is not received
  after a timer. TCP provides ordering by numbering packets.\\

  \textbf{UPD}
  \begin{itemize}
    \item Error checking
    \item Connectionless (packets are not stored for retrasmission. If transmission fails
          they are lost)
    \item no guarantee of delivery, ordering, or duplicate protection
    \item Sequence number and timestamp
    \item buffer client side (to compensate jitter and to reorder)
    \item configurable loss policy
  \end{itemize}

  \section{Example: Video streaming protocol on top of IP}
  What we need:
  \begin{itemize}
    \item Smooth video reproduction with constant play time, no pauses, best resolution
          possible, audio and video in synch.
    \item Minimize the bandwith used on the backbone of the internet.
  \end{itemize}
  How to solve:
  \begin{itemize}
    \item UDP at transport layer: implements a queue at receiver to handle packet
          reordering (datagrams are numbered for reordering at receival), delay (the
          receiver can tolerate delay until the queue has datagrams to play) and to
          absorb the jitter.
    \item We tolerate some lost packets. If the ratio of lost packets exceedes a constant
          we decrease quality (we download from the server a lower quality video and
          audio to decrease bitrate). If this is not enough we can resize the queue to
          tolerate higher delays.
    \item To synchronize audio and video even if we send them separately we append to
          each datagram the presentation timestamp, so the receiver can maintain one
          playout clock and schedule video and audio together.
    \item To avoid traffic on the backbone (and thus risking high latency and higher
          network costs) we can divide the content in cacheable segments and store them
          in proxies close to the clients.
  \end{itemize}

  \section{challenges of DS}
  \begin{itemize}
    \item \textbf{Heterogeneity: } Different OS, different data rapresentation, different implementation of standards (ex Posix thread
          vs windows threads).
    \item \textbf{Openess:} Use standards and simplicity as much as possible.
    \item \textbf{Security:} guarantee CIA (Confidenciality, Integrity, Availability). Still problems in mobile code application, DOS, Secure content based routing.
    \item \textbf{To ensure scalability use:} Asynchronous communication, caching and replication, epidemic dissemination, hierarchical structuring.
    \item \textbf{Failure handling:} Crash of a remote server is impossible to determine ceirtainly
          (server might be up but we are partitioned or messages are still arriving), we use suspect of crash.
          Replication mitigates this problems, but is difficult to ensure.
    \item \textbf{Transparency:} Mask network to ease development
    \item \textbf{Timing and event ordering:} There is no global clock or shared memory, so we need to use
          protocols to communicate betweeen nodes and synchronize
    \item \textbf{Partial failures:} Is impossible to find with ceirtainty failures. We can only make hypothesis based on missing responses.
  \end{itemize}
  \chapter{Modelling a distributed system}
  \section{Software Architecture}
  Where do we handle the distributed part? Application vs Middleware

  \subsection{Network OS Based Systems}
  Application uses communication services provided at OS level. Platform
  differences are handled by the application.

  \subsection{Middleware-Based Systems}
  Application relies on middleware for communication. Services commonly provided
  by the middleware are:
  \begin{itemize}
    \item Comunication services (sync/async, point to point/multicast).
    \item Mask OS differences
    \item Application services: distributed transactions, messaging, notification.
    \item Management services: naming, security, failure handling.
  \end{itemize}

  \section{Run-Time Architecture}
  What are the components and how is data exchanged at runtime?
  \section{Architectural Styles}
  \begin{itemize}
    \item \textbf{Client-server}: Clients query the API of the server with messages or RPC. Clients start the
          communication. Architecture is divided in tiers, each tier
          is on a different machine (or group of machines) and provides a service.
    \item \textbf{Service Oriented Architecture (SOA):} Differently from client server,
          the client know what service he needs but doesn't know the endpoint, so contacts
          a broker to obtain it.
          Built on the concept of Broker, Client and service provider.
          Clients query broker for a server and broker returns address of the service
          provider. Service provider exposes an interface and client queries it with the
          communication protocol.
          In case of complex operations that span across multiple web services we need
          orchestration: the process of calling a set of services in order to reach a goal.\\

          One implementation is web services (comunication happens through HTTP) (do not
          confuse with microservices that is an architectural pattern at the application
          level). The discovery of the service happens trough Web Services Discovery.
          UDDI (Universal Description Discovery Integration) is an XML registry that
          allows to search for a service defined with the WSDL (Web Service Description
          Language). Then communication happens in a RPC style over SOAP protocol. This
          tecnique isn't used anymore because is difficult to secure since anyone could
          impersonate a service and register on the UDDI.\\

          Service discovery is still useful to find the exact IP/port address of a
          replicated service. In this scenario the query is not hard (we know the service
          that we need, so the discovery part is just a query to a map). In case of
          client side service discovery the client contacts the discovery registry and
          asks for a service. The registy (that is a database that monitors the state of
          the available services and their location) returns the active instances and
          then the client decides what to query based on some load balancing policy. In
          case of server side service discovery the client queries a load balancer at a
          well known address that then forwards the request to the requested
          microservice. To keep track of their location the load balancer (server)
          queries the discovery registry.
    \item \textbf{REST:}

          Representational State Transfer is a communication protocol used over HTTP. It
          is based on URLs to indicate univocally resources. URLs are translated by the
          DNS service in actual IP/Port addresses. Each message can be of type POST GET
          PUT and DELETE and a few others. The responses are caracterised by a three
          digit code that indicates success or error.
          \begin{itemize}
            \item Client Server and stateless (any state must be transferred from the client to
                  the server).
            \item Data needs to be labelled as cacheable or not.
            \item Communication happens by transferring state rapresentation through HTTP
                  messages.
            \item Interfaces are uniform, meaning that everything has an id (URL), even actions.
                  State is transferred with a rapresentation (JSON or XML). Action requested is
                  described in the message payload and type. Clients move through states using
                  hyperlinks.
          \end{itemize}
    \item Peer-to-peer
    \item Object-oriented
    \item Data-centered (tuple spaces)
    \item \textbf{Event-based:}
          \begin{itemize}
            \item Components publish/subscribe to events.
            \item Message based,Asynchronous, multicast, anonymous.
          \end{itemize}
    \item \textbf{Mobile code}:

          Mobile code is an architecture that allows the relocation of components of a
          distributed system at runtime. We have the following cases:
          \begin{itemize}
            \item \textbf{Remote evaluation}: In this paradigm, a client ships code (a function, script,
                  or job specification) to a remote host. The remote machine executes the code
                  using its own data and execution environment, and returns the results to the
                  sender. Examples are google colab, AWS lambda, MapReduce tasks.
            \item \textbf{Mobile agent}: A mobile agent is an active entity that migrants across hosts
                  carrying its code and its execution state. The agent suspends execution,
                  relocates to another node, and resumes from the exact point it stopped.
                  Examples are an agent that moves between servers to collect data (this is done to move the
                  computation close to data to avoid expensive data migration).
            \item \textbf{Code on Demand:} The client already has state and execution capabilities but is
                  missing some code. It requests code from a remote server and executes it
                  locally. Examples are JavaScript downloaded and executed by a browser.
          \end{itemize}
          Code on Demand makes it easy to ship patches and new features to clients without the need of requesting updates.
  \end{itemize}

  \subsection{Peer-to-Peer (P2P)}
  \begin{itemize}
    \item No distinction between client/server.
    \item Resource sharing at the network edge.
  \end{itemize}

  \subsection{Object-Oriented Model}
  \begin{itemize}
    \item Components encapsulate data + API.
    \item Interaction via RPC.
    \item Pros: information hiding, reusability, load distribution.
  \end{itemize}

  \subsection{Data-Centered Model}
  \begin{itemize}
    \item Shared passive repository.
    \item Access via RPC, synchronized.
  \end{itemize}

  \subsection{Linda Tuple Spaces}
  \begin{itemize}
    \item \textbf{out(t)}: Write tuple.
    \item \textbf{rd(p)}: Read matching tuple (blocking).
    \item \textbf{in(p)}: Remove matching tuple.
    \item High decoupling, but scaling and reactivity are challenging.
  \end{itemize}

  \section{Interaction Model}

  \subsection{Synchronous Systems}
  \begin{itemize}
    \item Known bounds on process steps, message delays, and clock drift.
  \end{itemize}

  \subsection{Asynchronous Systems}
  \begin{itemize}
    \item No bounds on timing.
  \end{itemize}

  \subsection{Pepperland Example}
  \begin{itemize}
    \item \textbf{Leader election:} possible in async.
    \item \textbf{Simultaneous charge:} impossible in async.
    \item In sync systems: difference bounded by (max-min) delay.
  \end{itemize}

  \section{Failure Model}

  \subsection{Failures Types}
  \begin{itemize}
    \item \textbf{Omission:}
          \begin{itemize}
            \item Process: fail-stop vs crash.
            \item Channel: send/receive omission.
          \end{itemize}
    \item \textbf{Byzantine:}
          \begin{itemize}
            \item Arbitrary behavior or malicious faults.
          \end{itemize}
    \item \textbf{Timing:}
          \begin{itemize}
            \item Violation of timing constraints (only in sync systems).
          \end{itemize}
  \end{itemize}

  \subsection{Failure Detection}
  \begin{itemize}
    \item In sync systems: use timeouts.
    \item In async systems: can't distinguish failure from delay.
  \end{itemize}

  \section{Consensus in Failing Systems}

  \subsection{Two Generals Problem}
  \begin{itemize}
    \item Agreement requires at least one message.
    \item Last message uncertainty makes it unsolvable in async systems.
  \end{itemize}

  \subsection{FLP Theorem (Fischer, Lynch, Patterson 1985)}
  \begin{itemize}
    \item Consensus is impossible in async systems with even one failure.
  \end{itemize}

  \subsection{Real-Life Impact}
  \begin{itemize}
    \item Transactions (commit/abort).
    \item Sensor agreement.
    \item Fault detection.
  \end{itemize}

  \subsection{Solutions}
  \begin{itemize}
    \item Change assumptions (reliable links).
    \item Use probabilistic guarantees.
  \end{itemize}

  \chapter{Communication Fundamentals}

  \subsection{OSI Model (Recap)}
  \begin{itemize}
    \item \textbf{Physical, Data Link, Network:} low-level (bits, frames, routing).
    \item \textbf{Transport:} end-to-end (TCP, UDP).
    \item \textbf{Application layers:} merged in practice.
  \end{itemize}

  \subsection{Middleware}
  \begin{itemize}
    \item Provides common services: (un)marshaling, naming, security.
    \item Supports transient/persistent and synchronous/asynchronous communication.
  \end{itemize}

  \section{Remote Procedure Call (RPC)}

  \subsection{RPC Basics}
  \begin{itemize}
    \item Hides remote comms behind procedure call abstraction.
    \item Uses \textbf{stubs} to package calls into network messages.
  \end{itemize}

  \subsection{Parameter Passing}
  \begin{itemize}
    \item \textbf{By value:} send data copy.
    \item \textbf{By reference:} problematic; often unsupported.
    \item \textbf{By copy-restore:} alternative to reference. Copy in the value, copy out.
          Simulates the passing by reference because all the changes will be reflected into the original object.
  \end{itemize}

  \subsection{Marshalling \& IDL}
  \begin{itemize}
    \item \textbf{Marshalling:} flatten data into byte streams.
    \item \textbf{IDL:} defines service interface independent of language.
  \end{itemize}

  \subsection{RPC Implementations}
  \textbf{Sun RPC}
  \begin{itemize}
    \item Uses XDR, supports TCP/UDP.
    \item Only pass-by-copy; 1 input + 1 output param.
  \end{itemize}

  \textbf{DCE RPC}
  \begin{itemize}
    \item Multiple invocation semantics (at-most-once, idempotent, broadcast).
    \item Adds directory service and security (Kerberos).
  \end{itemize}

  \subsection{Binding}
  \begin{itemize}
    \item Find out where the server process is
    \item Find out how to establish communication with it
  \end{itemize}

  \section{Sun's Solution (Portmap)}

  \subsection{How it Works}

  \begin{itemize}
    \item Each server picks an available port and registers it with the local
          \texttt{portmap} daemon.
    \item Registration includes: \texttt{<service identifier, port>}.
    \item A client contacts \texttt{portmap} to retrieve the server's port number.
  \end{itemize}

  \subsection{Limitations}

  \begin{itemize}
    \item \texttt{portmap} only solves the \textbf{second problem}: how to communicate.
    \item The client must already know \textbf{where the server is}.
    \item No location transparency.
  \end{itemize}

  \subsection{Workarounds}

  \begin{itemize}
    \item The client can \textbf{multicast} queries to multiple \texttt{portmap} daemons.
    \item For large-scale systems, this is inefficient.
    \item More complex solutions require external \textbf{directory services}.
  \end{itemize}

  \section{DCE's Solution (Directory Service)}

  \subsection{Main Idea}

  \begin{itemize}
    \item The \textbf{DCE daemon} works like \texttt{portmap}, but is coupled with a
          \textbf{directory server}.
    \item The directory server (binder daemon) provides \textbf{location transparency}.
  \end{itemize}

  \subsection{Workflow}

  \begin{enumerate}
    \item The server picks a port and registers it with the local DCE daemon.
    \item The DCE daemon registers the service with the \textbf{directory server},
          including location and endpoint.
    \item The client contacts the directory server to \textbf{discover the server's
            location}.
    \item The client contacts the DCE daemon on the server machine to get the endpoint.
    \item The client performs the RPC call.
  \end{enumerate}

  \subsection{Advantages}

  \begin{itemize}
    \item The client only needs to know the directory service location, not the service
          location.
    \item Supports \textbf{location transparency} and \textbf{dynamic discovery}.
    \item The directory service can be \textbf{distributed} for scalability.
    \item Step 3 (lookup) is needed only once per session.
  \end{itemize}

  \section{Comparison}

  \begin{itemize}
    \item \textbf{Sun RPC (portmap):}
          \begin{itemize}
            \item Only resolves the port; no location service.
            \item Suitable for local services or LAN environments.
          \end{itemize}
    \item \textbf{DCE RPC:}
          \begin{itemize}
            \item Provides both name resolution and location binding.
            \item Better suited for large distributed systems.
          \end{itemize}
  \end{itemize}

  \subsection{Dynamic Activation}
  \begin{itemize}
    \item Use \texttt{inetd} to start servers on demand.
  \end{itemize}

  \subsection{Lightweight RPC}
  \begin{itemize}
    \item For local calls: shared memory + context switch.
    \item Used in DCOM, .NET.
  \end{itemize}

  \subsection{Asynchronous RPC}
  To avoid wasting client resources.

  \subsection{Batched vs Queued RPC}
  \begin{itemize}
    \item Batch calls to reduce overhead.
    \item Queue calls in case of connection problems
  \end{itemize}

  \section{Remote Method Invocation (RMI)}
  Can pass remote references with copy restore. Need of passing the code of
  objects.

  \section{Message Oriented Communication}

  \subsection{Motivation}
  \begin{itemize}
    \item RPC = point-to-point, synchronous.
    \item Message passing = decoupled, asynchronous, multi-point.
  \end{itemize}

  \subsection{Communication Types}
  \begin{enumerate}
    \item Transiend and persistent.
    \item Synch and asynch.
  \end{enumerate}

  \section{MPI (Message Passing Interface)}

  \begin{itemize}
    \item High-level message passing for HPC.
    \item Groups of processes, collective operations (broadcast, reduce).
    \item No fault tolerance.
  \end{itemize}

  \section{Message Queuing (MOM)}
  Like an email system for programs

  \subsection{Point-to-Point Queues}
  \begin{itemize}
    \item Asynchronous, persistent communication.
    \item Components have input/output queues.
  \end{itemize}

  \subsection{Architecture}
  Peer to peer communication but brokers can be used to discover services.
  \begin{itemize}
    \item Overlay network to route packages.
  \end{itemize}

  \section{Publish/Subscribe}

  Message based communication model where each client can publish and subscribe
  to events. Events are notified to the subscribers by a dispatcher that can be
  centralized or distributed. Subscriptions are specified by a subscription
  language. Based on the expressiveness of such language this communication model
  can be divided in topic based or content based: an expressive language is able
  to perform mactching also on the fields in the content of the message and
  express more complicated queries. We trade a greater expressiveness with the
  complexity of the routing.

  This model is transient, asynchronous, multipointand and implicit because
  events are produced by publishers and delivered to the middleware without
  expecting any ack from subscribers. This makes communication implicit, since
  only the middleware keeps track of the subscribers.

  Distributed dispatchers can be of two types:
  \begin{itemize}
    \item \textbf{DHT}:

          Message brokers organize in a DHT. Each nodes handles topics with an hash less
          or equal that their position in the circle. For each topic keeps a list of
          subscribers and delivers messages to them either directly or through forwarding
          to other message brokers. Another approach for delivery might be building an
          overlay network: every node that forwards the subscription remembers that he
          received a subscription message coming from a ceirtain broker. An event of that
          type will be forwarded in the opposite direction. This implicitly builds an
          overlay tree with root in the broker that handles the topic.
    \item \textbf{Overlay network}:

          Message brokers organize themselves into an overlay network that can follow
          either a DAG or a Tree structure. In the DAG case, each broker maintains a
          local group of clients together with their subscriptions, and forwarding is
          performed using routing information that is progressively accumulated during
          the subscription phase. Events are then propagated along the DAG while avoiding
          cycles, guaranteeing that all interested subscribers eventually receive each
          matching message.

          In the Tree approach, brokers construct and maintain explicit forwarding trees
          for content-based routing. Different forwarding strategies can be used:
          \emph{Per-Source Forwarding} (PSF), where each source defines a shortest-path
          tree (SPT) and brokers keep predicates per outgoing edge; \emph{improved PSF}
          (iPSF), which exploits indistinguishable sources to reduce forwarding-table
          size; and \emph{Per-Receiver Forwarding} (PRF), where messages carry the set of
          receivers in the header and each broker partitions this set using its unicast
          routing table. These trees ensure loop-free propagation and allow the
          middleware to route each event only along branches that lead to interested
          subscribers. PSF is ideal if the number of sources is less than the number of
          sinks, viceversa for PRF.
  \end{itemize}

  \subsection{Path Construction Strategies}

  \textbf{Distance Vector (DV)}

  \begin{itemize}
    \item Each node constructs a \textbf{Shortest Path Tree (SPT)} by repeatedly
          exchanging distance information with its neighbors. Nodes do not know the
          global topology: they maintain only a \textbf{vector of distances} to all
          destinations and the \textbf{next hop} to reach them.
    \item Path construction is performed through two types of messages:
          \begin{itemize}
            \item \emph{request (config)} messages, used to trigger the propagation
                  of distance information;
            \item \emph{reply (config response)} messages, where each node reports
                  its current distance estimates toward all nodes.
          \end{itemize}
    \item Nodes update their tables according to the \textbf{Bellman-Ford rule},
          selecting the neighbor that offers the minimal distance. Eventually, each node
          converges to a stable SPT rooted at the source.
    \item DV is simple and lightweight but may suffer from well-known issues such as
          \textbf{count-to-infinity} and slow convergence after failures.
  \end{itemize}

  \textbf{Link-State (LS)}

  \begin{itemize}
    \item Each node gathers the \textbf{complete view of the network topology}. This is
          achieved by flooding \textbf{Link-State Packets (LSPs)} containing information
          about its directly connected neighbors and link costs.
    \item Once every node has received all LSPs, they all share the same global topology
          graph. Each node then computes its own SPT locally using \textbf{Dijkstra's
            algorithm} or other shortest-path procedures.
    \item LS supports flexible optimization criteria: links can be weighted by latency,
          bandwidth, reliability, or congestion metrics, enabling \textbf{fine-grained
            control} over routing decisions.
    \item LS converges quickly and avoids DV's count-to-infinity problem, at the expense
          of higher memory requirements and more complex control traffic (LSP flooding
          and periodic refreshes).
  \end{itemize}

  \section{Stream-oriented Communication}

  \subsection{Types of Streams}
  \begin{itemize}
    \item \textbf{Asynchronous:}  The data items in a stream are transmitted one after the
          other without any further timing constraints (apart ordering)

    \item \textbf{Synchronous:} – Synchronous: There is a max end-to-end delay for each unit in the data
          stream
    \item \textbf{Isochronous:} – Isochronous: There is max and a min end-to-end delay (bounded jitter)
  \end{itemize}

  \subsection{QoS Parameters}
  \begin{itemize}
    \item Bitrate, delay, jitter, setup time.
  \end{itemize}

  \subsection{DiffServ (Internet QoS)}
  \begin{itemize}
    \item Differentiated Services field in IP header.
    \item DSCP (6 bits), ECN (2 bits).
  \end{itemize}

  \subsection{Application-layer QoS}
  \begin{itemize}
    \item Buffering to control jitter.Control max jitter by sacrificing session setup
          time
    \item Forward error correction.
    \item Interleaving data to mitigate loss.
  \end{itemize}

  \chapter{Naming}

  \textbf{Entities and Access Points}
  \begin{itemize}
    \item Names identify entities (hosts, users, files, etc.).
    \item Entities are accessed through addresses (access points).
    \item Use \textbf{location-independent names} to avoid issues with mobility.
  \end{itemize}

  \textbf{Identifiers}
  \begin{itemize}
    \item Never change during entity’s lifetime.
    \item One-to-one mapping with entities.
  \end{itemize}

  \textbf{Name Resolution}
  \begin{itemize}
    \item Map name $\rightarrow$ address (access point).
    \item Examples: DNS, LDAP, RMI registry, UDDI.
  \end{itemize}

  \section{Flat Naming}

  \subsection{Simple Solutions}
  \begin{itemize}
    \item \textbf{Broadcast}: like ARP; not scalable.
    \item \textbf{Multicast}: reduce traffic vs broadcast.
    \item \textbf{Forwarding Pointers}: leave reference at old location.
  \end{itemize}

  \subsection{Forwarding Pointers}
  \begin{itemize}
    \item Proxies forward requests.
    \item Chains may become long; chain reduction needed.
  \end{itemize}

  \subsection{Home-based Approaches}
  \begin{itemize}
    \item One home node maintains location.
    \item Used in Mobile IP and cellphone networks.
    \item Issues: latency, scalability, permanent home dependency.
  \end{itemize}

  \subsection{DHT}
  Flat naming can be implemented with a DHT where the name is the key and the
  address is the value.

  \section{Hierarchical Approaches}

  \textbf{Hierarchical Lookup}
  \begin{itemize}
    \item Root has entries for all entities.
    \item Look local $\rightarrow$ parent $\rightarrow$ child.
  \end{itemize}

  \textbf{Updates}
  \begin{itemize}
    \item Insert bottom-up or top-down.
    \item Deletion stops when multiple child paths exist.
  \end{itemize}

  \textbf{Issues}
  \begin{itemize}
    \item Caching useful for mobility patterns.
    \item Scalability: root becomes bottleneck.
  \end{itemize}

  \section{Structured Naming}

  \textbf{Name Spaces}
  \begin{itemize}
    \item Labeled graph with directories and leaves.
    \item Pathnames: e.g., \texttt{/alpha/beta/gamma}.
    \item Hard links and symbolic links possible.
  \end{itemize}

  \subsection{Name Resolution Modes}
  \begin{itemize}
    \item \textbf{Iterative}: client controls steps.
    \item \textbf{Recursive}: server resolves entire path.
  \end{itemize}

  \textbf{Caching \& Replication}
  \begin{itemize}
    \item TTL for cache validity.
    \item Root servers replicated.
  \end{itemize}

  \subsection{DNS and Mobility}

  \begin{itemize}
    \item If entity moves within domain: update local DNS.
    \item If entity moves across domains: use forwarding (similar to pointers).
    \item Increases lookup time; further updates not localized.
  \end{itemize}

  \section{Attribute-based Naming}

  \subsection{Directory Services}
  \begin{itemize}
    \item Query by attribute, not by name.
    \item Multiple entities may match query.
  \end{itemize}

  \subsection{LDAP}
  \begin{itemize}
    \item Hierarchical + attribute-based.
    \item Records are \texttt{<attribute, value>} pairs.
    \item Use \textbf{Relative Distinguished Name (RDN)} for unique IDs.
    \item Directory Information Base (DIB), structured as Directory Information Tree
          (DIT).
  \end{itemize}

  \subsection{Large-Scale LDAP}
  \begin{itemize}
    \item DIT partitions DIB.
    \item Directory Service Agents (DSA) manage parts of tree.
    \item Active Directory is LDAP-based.
  \end{itemize}

  \subsection{UDDI}
  \begin{itemize}
    \item Used for Web services discovery (similar to LDAP).
  \end{itemize}

  \section{Removing Unreferenced Entities}

  \textbf{Why?}
  \begin{itemize}
    \item Prevent stale references.
    \item Needed in distributed object platforms (e.g., RMI).
  \end{itemize}

  \subsection{Reference Counting}
  \begin{itemize}
    \item Object tracks number of references.
    \item If we are passing a reference and than deleting our reference, we need to first
          be sure that the passing was successful and then delete (otherwise the object
          will be deleted before the other one receives the new reference).
  \end{itemize}

  \subsection{Weighted Reference Counting}
  \begin{itemize}
    \item Counters per proxy.
    \item Only decrements sent.
    \item Limits total number of references.
  \end{itemize}

  \subsection{Reference Listing}
  \begin{itemize}
    \item Keep list of proxy identities.
    \item Idempotent insert/delete.
    \item Can use unreliable comms.
    \item Used in Java RMI (with leases and UDP).
  \end{itemize}

  \subsection{Mark and Sweep}
  \begin{itemize}
    \item Mark reachable entities; sweep the rest.
    \item Works poorly at scale due to global knowledge needed.
  \end{itemize}

  \subsection{Distributed Mark and Sweep (Emerald)}
  \begin{itemize}
    \item Proxies, skeletons, and objects initially white.
    \item Mark from roots and propagate via proxies.
    \item Requires global stability $\rightarrow$ scalability issue.
  \end{itemize}

  \chapter{Synchronization}

  phisical clocks drift from accurate time.
  \begin{itemize}
    \item \textbf{Drift Rate:} $\rho$
    \item \textbf{Max Drift Allowed:} $\delta$
    \item \textbf{Resync Period:} $T = \frac{\delta}{2\rho}$
  \end{itemize}

  so every resynch period they need to adjust either:
  \begin{itemize}
    \item Against external clock (GPS, NTP).
    \item Amongst clocks (Berkeley's algorithm).
  \end{itemize}

  \textbf{GPS Synchronization}

  \begin{itemize}
    \item Use atomic clocks on satellites.
    \item Requires solving 4 equations: position and time.
    \item Needs signals from 4 satellites.
  \end{itemize}

  \textbf{Christian's Algorithm}

  \begin{itemize}
    \item Client asks time server.
    \item Adjusts time by adding half of round trip time.
    \item RTT is supposed to be small wrt the required accuracy.
    \item To avoid the clock to run backward the time change might be introduced
          gradually by shortening the duration of each second until convergence.
  \end{itemize}

  \textbf{Berkeley's Algorithm}

  \begin{itemize}
    \item Time server collects all clocks.
    \item Computes average and broadcasts it.
  \end{itemize}

  \textbf{NTP (Network Time Protocol)}

  Is a protocol used to share time over the internet. Time servers are organized
  in strata based on their vicinity to a precise time source: strata 1 servers
  contain a precise time source (GPS) and send data to lower strata over the
  internet. The lower the strata the higher the precision (1ms if in LAN there is
  a strata 1) (10-50ms over WAN). To synchronize we use:

  \begin{itemize}
    \item \textbf{Periodic multicast}: from NTP server to clients in LAN.
    \item \textbf{RPC} (like Christian)
    \item \textbf{Symmetric} for greater precision at the cost of double the messages.
  \end{itemize}
  % TODO Learn Symmetric

  \section{Simulating time using logical clocks and preserving event ordering}

  \subsection{Happens Before Relation}

  \begin{itemize}
    \item $e \rightarrow e'$ if same process and $e$ before $e'$.
    \item Or $e$ sends message, $e'$ receives it.
  \end{itemize}

  \subsection{Lamport Scalar Clocks}

  \begin{itemize}
    \item Each process has a counter.
    \item On send: increment counter.
    \item On receive: counter = $\max(local, received) + 1$
  \end{itemize}

  \subsection{Totally Ordered Multicast}

  \begin{itemize}
    \item Messages stored in Lamport-ordered queues.
    \item Broadcast messages and acks.
    \item Deliver message only if at top and acked by all.
  \end{itemize}

  \subsection{Vector Clocks}

  \begin{itemize}
    \item Each node has a vector of size $N$.
    \item On event: increment own entry.
    \item On receive: take element-wise max.
    \item Defines partial order (concurrent if not comparable).
  \end{itemize}

  \subsection{Group Chat Example}

  \begin{itemize}
    \item Order messages that are causally related.
    \item Concurrent messages can arrive in any order.
  \end{itemize}

  \section{Mutual Exclusion}

  \subsection{Centralized Coordinator}

  \begin{itemize}
    \item Ask the server for permission.
    \item Problem: Single point of failure.
  \end{itemize}

  \subsection{Mutual excl with scalar clocks}

  \begin{itemize}
    \item Each node sends request with Lamport clock.
    \item Wait for acks from all others.
    \item Fair (clock-based), safe, but blocks on failure.
  \end{itemize}

  Upon receipt of m, a process Pj:

  \begin{itemize}
    \item If it does not hold the resource and it is not interested in holding the
          resource, Pj sends an acknowledgment to Pi
    \item If it holds the resource, Pj puts the requests into a local queue ordered
          according to Tm (process ids are used to break ties)
    \item If it is also interested in holding the resource and has already sent out a
          requests, Pj compares the timestamp Tm with the timestamp of its own requests.
          If Tm is the lowest one, Pj sends an acknowledgement to Pi, otherwise it put
          the request into the local queue above
  \end{itemize}

  \subsection{Token Ring}

  \begin{itemize}
    \item Only node with token can access resource.
    \item Safe, live but not fair (ring order).
    \item Failure detection needed for crash/token loss.
  \end{itemize}

  \section{Leader Election}

  \subsection{Hypotheses}

  \begin{itemize}
    \item Closed system, unique IDs.
    \item Synchronous system (can detect failures).
  \end{itemize}

  \subsection{Bully Algorithm}

  \begin{itemize}
    \item Node with highest ID wins.
    \item The node that finds out about the failure starts an election by sending
          messages to the nodes with higher ids.
    \item If a node receives an election message it means that it has an higher id, so it
          will start a new election.
    \item If there is no one active with an higher id inform everyone of the new leader.
    \item Complexity: number of messages: worst $O(n^2)$, best $O(n)$.
  \end{itemize}

  \subsection{Ring Election}

  \begin{itemize}
    \item Nodes pass IDs in a ring. Append its own id to the array passed around.
    \item When array already contains id, decide in a deterministic way for the leader.
    \item Complexity: number of messages: worst $O(n^2)$, best $O(n)$.
    \item Can optimize to $O(n\log n)$.
  \end{itemize}

  \section{Global State Collection}

  \subsection{Consistent Cut}

  \begin{itemize}
    \item A cut contains all events that happened before included events.
  \end{itemize}

  \subsection{Distributed Snapshot (Chandy-Lamport)}

  \begin{itemize}
    \item Initiator records state, sends tokens.
    \item On first token: record state, send tokens.
    \item Record messages in transit.
    \item Proof: Always produces a consistent cut.
  \end{itemize}

  \subsection{Termination Detection (Dijkstra-Scholten)}

  \begin{itemize}
    \item Use distributed tree of activations.
    \item When a node starts a computation it will send a message to the nodes he
          activates. If the nodes where already active they will immediately respond with
          an ack, else they will be considered as children.
    \item Acks sent when children finish.
    \item Snapshot can also detect termination by checking idle + no messages.
  \end{itemize}

  \section{Distributed Transactions}

  \subsection{ACID Properties}

  \begin{itemize}
    \item Atomic, Consistent, Isolated, Durable.
  \end{itemize}

  \subsection{Single Node}

  \begin{itemize}
    \item \textbf{Pessimistic:} private copy, commit updates.
    \item \textbf{Optimistic:} log changes, rollback on abort.
  \end{itemize}

  \subsection{2PL}
  Get all the locks before starting the transactions.\\

  three possible approaches:
  \begin{itemize}
    \item Centralized lock server. This is a single point of failure, needs to be
          replicated.
    \item Primary lock per object. Transaction manager will interact with the data
          manager that has the primary lock.
    \item Fully distributed locks $\rightarrow$ requires consensus.
  \end{itemize}

  \subsection{Timestamp Ordering}

  \textbf{Pessimistic:}

  Every process keeps the logical time (Lamport clock) of the last committed read
  and write. When a new write arrives if it has a timestamp greater than the
  saved read and write, then is write in a tentative way waiting for the commit.
  Else it is discarded because it arrived too late. When a new read arrives, if
  it has a timestamp greater than the last write, then the process gives the
  value in the greatest write between the committed and not that has the
  timestamp smaller than the new read, if it is committed, else will give it when
  committed. If the write aborts then the process is repeated. If the new read
  has a timestamp smaller than the last read is discarded

  \textbf{Optimistic:}

  In this case the data items are stamped with the start of the transaction. If
  the transaction finishes and there are items that changed from the beginning,
  then the whole transaction will be aborted and a rollback will be performed.

  \section{Distributed deadlock detection}

  \subsection{Detection}
  Centralized detection is not feasible

  \begin{itemize}
    \item Chandy-Misra-Haas: Send probe. Loop $\rightarrow$ deadlock.
  \end{itemize}
  The Chandy-Misra-Haas distribute deadlock detection works simply by sending a
  probe when a proces thinks that there is a deadlock. The probe will be forwarded by
  all processes. If there is actually a deadlock the probe will make it back to the starting
  Process and this means that there is a deadlock.
  To solve a deadlock, the process that finds out can commit suicide, or in the probe
  processes might write an id and the process with the higher id gets killed.

  \subsection{Prevention}
  When getting a shared resource compare the timestamp with the timestamp of
  whoever is holding the resource.
  \begin{itemize}
    \item Use timestamps: older transaction kills younger.
    \item Preemption: older can stop younger and retry later.
  \end{itemize}

  \chapter{Fault tolerance}

  \textbf{Why Fault Tolerance?}
  \begin{itemize}
    \item \textbf{Availability}: system ready when needed.
    \item \textbf{Reliability}: system runs for long time.
    \item \textbf{Safety}: no catastrophic effects.
    \item \textbf{Maintainability}: easy to repair.
    \item \textbf{Availability $\neq$ Reliability}: 99.999\% uptime $\neq$ reliable.
  \end{itemize}

  \textbf{From Faults to Failures}
  \begin{itemize}
    \item Failure ← Error ← Fault.
    \item Fault-tolerant = still works under faults.
  \end{itemize}

  \textbf{Fault Types}
  \begin{itemize}
    \item \textbf{Transient}: one-time.
    \item \textbf{Intermittent}: irregular.
    \item \textbf{Permanent}: continuous.
  \end{itemize}

  \textbf{Failure Models}
  \textbf{Omission Failures}
  \begin{itemize}
    \item Process: fail-stop, fail-silent.
    \item Channels: packets dropped.
  \end{itemize}

  \textbf{Timing Failures} (synchronous systems).

  \textbf{Byzantine Failures}
  \begin{itemize}
    \item Arbitrary actions/messages.
  \end{itemize}

  \textbf{Redundancy Techniques}
  \begin{itemize}
    \item \textbf{Information}: Hamming codes.
    \item \textbf{Time}: retry.
    \item \textbf{Physical}: hardware duplication.
    \item \textbf{TMR}: triple modular redundancy + voting.
  \end{itemize}

  \section{Client-Server Tolerance}

  \textbf{RPC Failures}
  \begin{itemize}
    \item Server not found, lost request/reply, crashes.
    \item Cannot distinguish lost reply from crash.
    \item Semantics: at-most once, at-least once.
  \end{itemize}

  \textbf{Print Server Example}
  \begin{itemize}
    \item Client reissue strategies: always, never, only if reply, only if no reply.
    \item No strategy guarantees exactly-once semantics.
  \end{itemize}

  \textbf{Client Crashes}
  \textbf{Orphan computation solutions}:
  \begin{itemize}
    \item Extermination: client logs calls and when reboot kills (problem with orphans)
    \item Reincarnation: Client reboots and sends messages with new epoch. Computations
          started by the client but with old epoch are discarded.
    \item Gentle Reincarnation: Same or reincarnation but the server kills orphans if
          cannot locate client.
    \item Expiration: Computation expires
  \end{itemize}

  \section{Process Resilience \& Agreement}

  \textbf{Redundant Groups}
  \begin{itemize}
    \item Process groups (flat/coordinated).
    \item Fail-silent: $k+1$ for $k$-fault-tolerance.
    \item Byzantine: $2k+1$ needed.
  \end{itemize}

  \textbf{Consensus Problem}
  \begin{itemize}
    \item \textbf{Agreement}: all agree.
    \item \textbf{Validity}: if same input $v$, then output $v$.
    \item \textbf{Termination}: all decide eventually.
  \end{itemize}

  \textbf{Crash Failures (sync)}
  \begin{itemize}
    \item Solvable in $f+1$ rounds for $f$ faults.
    \item FloodSet algorithm.
  \end{itemize}

  \textbf{FloodSet algorithm}
  \begin{itemize}
    \item Each process chooses v0 and puts it in W
    \item f+1 rounds of broadcast messages with content of W
    \item Upon receival of W merge with own W
    \item At least one round will not involve failures
    \item Decide with same deterministic function the value in W
  \end{itemize}

  \textbf{Byzantine Failures (sync)}
  \begin{itemize}
    \item Lamport's Byzantine Generals Problem.
    \item Need $3f+1$ nodes to tolerate $f$ faulty.
  \end{itemize}

  \textbf{FLP Theorem (async)}
  \begin{itemize}
    \item Consensus impossible with even 1 crash fault in async model.
  \end{itemize}

  \section{Reliable Group Communication}

  \textbf{Multicast Challenges}
  \begin{itemize}
    \item Sender crash mid-send.
    \item Dynamic membership.
  \end{itemize}

  \textbf{Non faulty processes}
  \begin{enumerate}
    \item Ack/Nack-based protocols (scalability issues).
    \item Suppress replicated NACKs
    \item Hierarchical control to improve scalability (needs to be maintained)
  \end{enumerate}

  \textbf{Atomic Multicast}
  \begin{itemize}
    \item All-or-none + same order for all.
  \end{itemize}

  \textbf{Virtual Synchrony}
  \begin{itemize}
    \item A view contains all the active processes.
    \item The view changes (failures of join) are notified in broadcast.
    \item All messages sent in a view are received by the same view.
  \end{itemize}

  \textbf{Ordering Models}
  \begin{itemize}
    \item \textbf{Unordered}, \textbf{FIFO}, \textbf{Causal}
    \item \textbf{Total}. Every member of the view receives messages in the same order.
    \item Atomic multicast = totally ordered virtual synchrony.
  \end{itemize}

  \textbf{Implementation of VS (ISIS)}
  Idea: messages sent to group view are delivered before the view change.
  \begin{itemize}
    \item Assumption: Reliable FIFO channels
    \item Messages buffered until stable.
    \item Message stable if every view member received it.
    \item Processes are notified of view change $\rightarrow$ stop sending new messages.
          Stabilize all messages by broadcasting them to the old view send a flush in
          multicast. When every process received the flush from every other process it
          means that the messages are stable and the new view can be installed.
  \end{itemize}

  (this assumes no failures during the view change). To allow that there is another version.

  \section{Recovery Techniques}

  Checkpointing and logging.

  \textbf{Recovery Line and consistent cut}
  Find a good checkpoint that corresponds to a valid global state: No message
  sent without receive by target and no message receive without send.

  \textbf{Independent Checkpointing}

  \begin{itemize}
    \item Each process saves state independently.
    \item May require multiple rollbacks: \textbf{Domino Effect}.
  \end{itemize}

  \includegraphics[width=\columnwidth]{Figures/dep.png}

  \textbf{Coordinated Checkpointing}
  Idea: block communication until everyone has the checkpoint.
  \begin{itemize}
    \item Coordinator sends \texttt{CHKP-REQ}.
    \item Processes take checkpoint, buffer messages, send \texttt{ACK}.
    \item On all ACKs, coordinator sends \texttt{CHKP-DONE}.
  \end{itemize}

  \textbf{Logging Schemes}

  \textbf{Principle}
  \begin{itemize}
    \item Replay messages from log after crash.
    \item Assumes messages cause deterministic actions on send and receive.
  \end{itemize}

  \textbf{Key Concepts}
  \begin{itemize}
    \item \textbf{Stable message}: written to stable storage.
    \item \textbf{DEP(m)}: processes dependent on $m$ (also indirectly, meaning that they
          received a message dependent on m).
    \item \textbf{COPY(m)}: processes holding a copy of $m$.
  \end{itemize}

  \textbf{Orphan Process}
  \begin{itemize}
    \item A surviving process $Q$ is orphan if $Q \in \text{DEP}(m)$ and all of
          $\text{COPY}(m)$ crashed.
    \item To avoid: ensure $\text{DEP}(m) \subseteq \text{COPY}(m)$.
  \end{itemize}

  We cannot rebuild the status of the crashed process because no one has the
  message that the orphan received. Thus the orphan needs to rollback the changes
  caused by the message.

  \textbf{Pessimistic vs Optimistic Logging}

  \textbf{Pessimistic Logging}
  \begin{itemize}
    \item Message logged before delivery.
    \item No orphans possible.
    \item Sender or receiver writes to stable storage before further communication.
  \end{itemize}

  \textbf{Optimistic Logging}
  \begin{itemize}
    \item Message logged asynchronously.
    \item Orphans may be created, must rollback to consistent state.
    \item Similar to independent checkpoint rollback.
  \end{itemize}

  \textbf{Final Notes}
  \begin{itemize}
    \item Pessimistic: safer, more overhead.
    \item Optimistic: lighter, more risky.
    \item Best: combine logging + coordinated checkpointing.
  \end{itemize}

  \chapter{Consensus}
  $Consensus=Agreement+Validity+Termination$

  \section{Atomic Commit Protocols}

  \subsection{Atomic Commit}
  \begin{itemize}
    \item Ensures \textbf{atomicity} (ACID): all or nothing.
    \item If distributed: either all nodes commit or abort.
    \item Crash in any node $\rightarrow$ global abort.
  \end{itemize}

  \subsection{Consensus vs Atomic Commit}
  \begin{itemize}
    \item \textbf{Consensus:} nodes propose a value, agree on one.
    \item \textbf{Atomic Commit:} all must vote commit for commit to happen.
    \item Consensus tolerates some failures; atomic commit does not.
  \end{itemize}

  \section{Two-Phase Commit (2PC)}

  \textbf{Phase 1: Prepare}
  \begin{itemize}
    \item Coordinator $\rightarrow$ \texttt{prepare} to all participants.
    \item Each replies with \texttt{vote-commit} or \texttt{vote-abort}.
  \end{itemize}

  \textbf{Phase 2: Commit/Abort}
  \begin{itemize}
    \item All commit votes $\rightarrow$ \texttt{global-commit}.
    \item Any abort $\rightarrow$ \texttt{global-abort}.
  \end{itemize}

  \textbf{Failures:}
  \begin{itemize}
    \item Coordinator crash $\rightarrow$ blocking.
    \item Participant crash $\rightarrow$ assume abort after timeout.
    \item \textbf{Blocking protocol}: no progress until coordinator recovers.
  \end{itemize}

  Recovery in case of failure of the Coordinator:\\ Participant Timeout
  $\rightarrow$ request decision from others.

  \section{Three-Phase Commit (3PC)}

  \begin{itemize}
    \item Adds \texttt{pre-commit} phase to avoid blocking.
    \item No state can directly lead to both commit/abort.
    \item Coordinator can safely act on timeouts.
  \end{itemize}

  \textbf{Phases:}
  \begin{itemize}
    \item \texttt{prepare} → \texttt{vote-commit} → \texttt{prepare-commit} → \texttt{ready-commit} → \texttt{global-commit}
  \end{itemize}

  \textbf{Failure Cases:}
  Participant fails:

  A participant fails
  \begin{itemize}
    \item  Coordinator blocked waiting for vote (Wait state) can assume abort decision (no
          participant can be in Pre-commit)
    \item Coordinator blocked in Pre-commit state can safely commit and tell the failed
          participant to commit when it recovers
  \end{itemize}

  Coordinator fails:
  \begin{itemize}
    \item \textbf{Quorum rules}: abort if one votes abort or is in init.
    \item Commit if majority ready and at least one precommit.
    \item Abort if majority in ready and no one in commit.
    \item Commit if one is in commit
  \end{itemize}

  \textbf{Properties:}
  \begin{itemize}
    \item \textbf{Synchronous system}: safe + live (non-blocking).
    \item \textbf{Asynchronous system}: may not terminate.
    \item More expensive than 2PC.
  \end{itemize}

  \subsection{Commit Protocol Summary}
  \begin{itemize}
    \item 2PC: safe, \textbf{blocking}.
    \item 3PC: safe, \textbf{non-blocking (synchronous only)}.
    \item 3PC rarely used in practice.
  \end{itemize}

  \subsection{CAP Theorem}
  \begin{itemize}
    \item \textbf{C}onsistency: single up-to-date copy.
    \item \textbf{A}vailability: always respond to reads/writes.
    \item \textbf{P}artition tolerance: tolerate network partitions.
    \item \textbf{Only 2 of 3} can be achieved at once.
    \item Systems balance C and A depending on needs.
  \end{itemize}

  \section{Replicated State Machines}

  \subsection{Overview}
  \begin{itemize}
    \item General-purpose consensus: multiple servers act as one.
    \item Maintain identical copies of state.
    \item Survive failures, provide continuous service.
  \end{itemize}

  \subsection{State Machine Model}
  \begin{itemize}
    \item Internal state updated by external requests.
    \item Example: key-value storage with \texttt{read/write}.
  \end{itemize}

  \subsection{Replicated Logs}
  \begin{itemize}
    \item Ensure all state machines execute same commands in same order.
    \item Achieved via consensus (e.g., Raft).
  \end{itemize}

  \subsection{Failure Model}
  \begin{itemize}
    \item Async comm: delays, loss, duplication possible.
    \item Crash-stop failures, recovery via durable log.
  \end{itemize}

  \subsection{Guarantees}
  \begin{itemize}
    \item \textbf{Safety}: all correct nodes have same sequence.
    \item \textbf{Liveness}: progress if majority is alive and connected.
    \item FLP theorem $\rightarrow$ no full guarantee in async model.
  \end{itemize}

  \section{Raft Protocol}

  \subsection{Motivation}
  \begin{itemize}
    \item Easier to understand and implement than Paxos.
    \item Decomposed into: log replication, leader election, safety.
  \end{itemize}

  \subsection{Server States}
  \begin{itemize}
    \item \textbf{Follower} (default), \textbf{Candidate}, \textbf{Leader}.
    \item Election triggered by heartbeat timeout.
  \end{itemize}

  \subsection{Leader Election}
  \begin{itemize}
    \item Randomized timeouts (150-300 ms).
    \item Candidate increases term, requests votes.
    \item Wins if majority grants vote.
    \item Safety: one vote per term, persist on disk.
    \item Servers with incomplete logs must not get elected. Server receiving a message
          from a server not up to date will deny the vote.
  \end{itemize}

  \subsection{Terms}
  \begin{itemize}
    \item Logical clock: one leader per term.
    \item Used to detect stale info and ensure safety.
  \end{itemize}

  \section{Normal Operation}

  \begin{itemize}
    \item Client → Leader: send command.
    \item Leader appends to log, sends \texttt{AppendEntries} to followers.
    \item Once majority ack: commit + apply to state machine.
    \item Followers apply committed entries.
    \item Retry mechanism for lagging nodes.
  \end{itemize}

  \section{Log Management}

  \subsection{Log Structure}
  \begin{itemize}
    \item Entries have term, index, command.
    \item Stored on disk for durability.
  \end{itemize}

  \subsection{Consistency}
  \begin{itemize}
    \item \textbf{Log matching property}: same index + term $\rightarrow$ same history.
    \item \texttt{AppendEntries} includes $<$prevIndex, prevTerm$>$.
          In case the follower has Mismatched log cause retries with decremented index. (this way we reconcile logs).
  \end{itemize}

  \subsection{Leader Completeness}
  \begin{itemize}
    \item If entry is committed, it must exist in future leaders.
    \item Election denied if candidate's log is less up-to-date.
  \end{itemize}

  \section{Client Interaction}

  \begin{itemize}
    \item Clients talk to leader.
    \item On crash: retry with new leader (discovered indirectly).
    \item Linearizability via unique client request IDs.
    \item Servers track last client ID to discard duplicates.
  \end{itemize}

  \section{Network Partitions}

  \begin{itemize}
    \item Split brain: two leaders may exist.
    \item Old leader can't commit (no majority).
    \item Once reconnected, stale leader steps down (learns new term).
  \end{itemize}

  \section{Use in Distributed DBMS}

  \begin{itemize}
    \item Partitions are replicated state machines.
    \item 2PC used across partitions for atomicity.
    \item Partition = group of nodes with Raft.
    \item Availability: majority of nodes in each partition must be reachable.
  \end{itemize}

  \section{Extras (not detailed)}
  \begin{itemize}
    \item Log compaction
    \item Membership changes
    \item Performance optimization
  \end{itemize}

  \subsection{Byzantine Fault Tolerance (BFT)}
  \begin{itemize}
    \item Extends state machine replication to tolerate faulty/malicious nodes.
    \item Requires $3f+1$ nodes to tolerate $f$ Byzantine faults.
    \item Similar assumptions as Paxos/Raft.
  \end{itemize}

  \subsection{Blockchain as a Replicated Log}
  \begin{itemize}
    \item Cryptocurrency = replicated state machine.
    \item \textbf{State:} user balances.
    \item \textbf{Ledger:} replicated log of signed transactions.
    \item Byzantine users may attempt double spending.
  \end{itemize}

  \subsection{Assumptions (e.g., Bitcoin)}
  \begin{itemize}
    \item Very large, open set of nodes.
    \item Participants not known in advance.
    \item No entity controls majority of compute.
  \end{itemize}

  \subsection{Guarantees}
  \begin{itemize}
    \item No provable safety; correctness holds with high probability.
    \item Stronger guarantees require assumptions on compute majority.
  \end{itemize}

  \subsection{Blockchain Structure}
  \begin{itemize}
    \item Public ledger of all transactions since genesis.
    \item Each block contains several transactions.
    \item Nodes broadcast new transactions; others validate and propagate.
    \item If private key is lost, funds are unrecoverable.
  \end{itemize}

  \subsection{Proof of Work (PoW)}
  \begin{itemize}
    \item Miner solves cryptographic puzzle over previous block + new data.
    \item \textbf{Hard to compute}, \textbf{easy to verify}.
    \item One valid solution every ~10 minutes (on average).
    \item Difficulty adapts to global compute power.
  \end{itemize}

  \subsection{Mining Process}
  \begin{itemize}
    \item Miners collect pending transactions, form a block, compute PoW.
    \item Incentivized: earn Bitcoin if successful.
    \item On success: broadcast block, others append and continue mining.
    \item Agreement on block order via longest chain.
  \end{itemize}

  \subsection{Forks \& Consistency}
  \begin{itemize}
    \item Two miners may solve PoW concurrently → fork.
    \item Chain that grows faster is accepted.
    \item No absolute certainty on finality (a longer chain may emerge later).
  \end{itemize}

  \subsection{Double Spending Attack}
  \begin{itemize}
    \item Malicious node with high compute can create two chains:
          \begin{itemize}
            \item Chain A: spend to Alice, broadcast.
            \item Chain B: spend same coin to Bob, broadcast separately.
          \end{itemize}
    \item A and B cannot both be sure theirs is the canonical chain.
  \end{itemize}

  \chapter{Consistency}

  \subsection{Replication goals}
  \begin{itemize}
    \item \textbf{Fault tolerance} via redundancy.
    \item \textbf{Availability} during disconnection (mobile) or overload.
    \item \textbf{Performance} via load balancing and reduced latency.
  \end{itemize}

  \section{Replication: Challenges}
  \begin{itemize}
    \item \textbf{Consistency} across replicas: updates must be coordinated.
    \item \textbf{Conflict handling} when concurrent writes occur.
    \item \textbf{Scalability/performance tradeoff}: synchronizing replicas is costly.
    \item Varying consistency needs: data-centric vs client-centric.
  \end{itemize}

  \section{Consistency Models}
  Contract between client and replicated system. Cannot be Read should return the
  latest write because without global clock, this is ambiguous.

  \subsection{Types of Guarantees}
  \begin{itemize}
    \item \textbf{Content}: max version difference across replicas.
    \item \textbf{Staleness}: max propagation delay.
    \item \textbf{Order}: constraints on update sequence.
  \end{itemize}

  \section{Single Leader Protocols}

  \begin{itemize}
    \item One replica is leader; all writes go to it.
    \item Followers get updates from leader.
    \item Reads can go to leader or followers.
  \end{itemize}

  \subsection{Write Strategies}
  \begin{itemize}
    \item \textbf{Synchronous}: wait for all followers.
    \item \textbf{Asynchronous}: commit after leader write.
    \item \textbf{Semi-synchronous}: wait for $k$ followers.
  \end{itemize}

  \subsection{Pros \& Cons}
  \begin{itemize}
    \item Safe: avoid write-write conflicts.
    \item Failover needs consensus.
    \item Read-write conflicts possible (e.g., async replica).
  \end{itemize}
  Used in Typical in single-org / data center (low latency).

  \section{Multi Leader Protocols}

  \begin{itemize}
    \item Clients write to different replicas.
    \item No global order of writes $\rightarrow$ write-write conflicts possible.
  \end{itemize}

  \subsection{When used}
  \begin{itemize}
    \item Geo-replication: writing to far-away leader is too costly.
    \item Single datacenter has single leader. Between datacenter multileader.
  \end{itemize}

  \section{Leaderless Protocols}

  \begin{itemize}
    \item Client or coordinator writes/reads to/from multiple replicas.
    \item Uses quorum protocols (majority voting).
  \end{itemize}

  \section{Data-Centric Consistency Models}

  \subsection{Sequential Consistency}
  \begin{itemize}
    \item Operations within a process may not be re-ordered (Each process sees operations
          in program order.)
    \item All processes see the same interleaving
    \item No real time notion.
  \end{itemize}

  \includegraphics[width=\columnwidth]{Figures/seq.png}

  \subsection{Single leader implementation}
  Synch replication. The clients communicate with the leader for writes but can
  read from every replica. The server decides the order of operations and sends
  broadcast messages to all the replica to maintain consistency of the state.

  Assumptions:
  \begin{enumerate}
    \item Sticky client (clients read always from the same source)
    \item FIFO channels
  \end{enumerate}

  \subsection{Leaderless implementation}
  Client contact a quorum of nodes to perform a read and a write. In particular
  the following conditions need to be valid

  \begin{enumerate}
    \item Se c'è una read allora il numero di nodi contattati per la read sommati al
          numero di nodi che sta performando una concurrent write, deve essere maggiore
          del numero di nodi (in questo modo ho almeno un nodo che riceve sia la read che
          la write e può decidere l'ordine) $Nr + Nw > N$
    \item Write con almeno la metà dei nodi (per evitare che partizioni più piccole del
          quorum abbiano le loro versioni dei dati). $ NW > N/2$
  \end{enumerate}

  \subsection{Notes}
  \begin{enumerate}
    \item High latency due to synchronous interactions
    \item Clients are blocked in the case of network partitions
    \item No high availability
  \end{enumerate}

  \subsection{Linearizability}
  \begin{itemize}
    \item Stronger than sequential.
    \item Each operation appears to occur at a unique point in wall-clock time (so
          everyone see the same order).
    \item Recency guarantee: once write completes, everyone sees it.
    \item Also called atomic or strong consistency.
    \item Harder to achieve with failures or partitions.
    \item Implementation with single leader synch system, with a blocking system: When
          The leader needs to write, will send out lock messages to lock the variable in
          every replica.
    \item No sticky client needed!
  \end{itemize}

  \subsection{Causal Consistency}
  \begin{itemize}
    \item Preserves causal relationships (Lamport's happens-before).
    \item All processes see causally related writes in same order.
    \item Concurrent writes may be seen in any order.
    \item Highly available: clients can proceed even if isolated.
    \item Requires sticky clients.
  \end{itemize}
  \subsection{Implementation}
  Each write is timestamped with a vector clock. The writes arrive in broadcast.
  When a write is received, the process will use the vector clock to understand
  if there are some writes that have been missed.

  \subsection{FIFO Consistency (PRAM)}
  \begin{itemize}
    \item Writes from a single process are seen in order (also if they happen on
          different variables).
    \item No ordering guaranteed between different writers.
    \item Very easy to implement, even with multi-leader (if channels are FIFO, you just
          need to broadcast your updates, if channels are not FIFO, than it is necessary
          to timestamp with scalar clocks all the messages and when a node receives a
          message it will wait for all the prev from that node)
    \item Updates tagged with increasing sequence numbers.
  \end{itemize}

  \subsection{Property}
  If sequentially consistent it will also be fifo and causally consistent.

  \subsection{Esercizio: capisci se è sequentially consisten con 2 variabili}
  Cerca di trovare un'esecuzione lineare

  \subsection{Eventual Consistency}
  \begin{itemize}
    \item All replicas converge eventually.
    \item Used when updates are rare or can be easily resolved.
    \item Suitable for web caches, DNS, social media.
    \item Lightweight, highly available.
  \end{itemize}

  \subsection{Conflict-Free Replicated Data Types (CRDTs)}
  \begin{itemize}
    \item Ensure convergence even with concurrent, unordered updates.
    \item Commutative operations (e.g., counters).
    \item Example: list with timestamped appends (last-write-wins).
  \end{itemize}

  \section{Client-Centric Consistency Models}

  \subsection{Motivation}
  \begin{itemize}
    \item Clients may access different replicas.
    \item Goal: define guarantees from the perspective of one client.
  \end{itemize}

  Think about the case of modifying a list by appending stuff. To do this we need
  to propagate changes to every replica before accepting new operations from that
  client.

  \subsection{Monotonic Reads}
  \begin{itemize}
    \item Once a client reads a value, future reads return same or newer.
    \item Prevents reading older values on other replicas.
  \end{itemize}
  Means that if I read that the list contains 1 and 2, a subsequent read from
  other replicas needs to contain 1 and 2, or a newer value.

  \subsection{Monotonic Writes}
  \begin{itemize}
    \item Writes by a client are seen in issue order.
    \item Similar to FIFO, limited to same client.
  \end{itemize}

  \subsection{Read Your Writes}
  \begin{itemize}
    \item A client always sees the effect of its own writes.
    \item Ex: password change, form submission.
  \end{itemize}

  \subsection{Writes Follow Reads}
  \begin{itemize}
    \item Writes happen on a state at least as recent as prior reads.
    \item Ex: reply to post after reading original.
  \end{itemize}

  \section{Implementation}

  \begin{itemize}
    \item Each operation has unique ID: replica ID + seq. number.
    \item \textbf{Read-set}: write IDs seen in reads.
    \item \textbf{Write-set}: write IDs created.
    \item When the client switches replicas it will inform the new replica of the Read
          set and write set. The replica will wait for the necessary info to arrive
    \item Vector clocks used to track causal dependencies.
  \end{itemize}

  \subsection{Strategies}
  \begin{itemize}
    \item \textbf{Monotonic reads}: ensure new replica has all writes in read-set.
    \item \textbf{Monotonic writes}: ensure new replica has all writes on write-set.
    \item \textbf{Read your writes}: same as monotonic writes.
    \item \textbf{Writes follow reads}: fetch updates in read-set before write.
  \end{itemize}

  \section{Replica Placement}

  \begin{itemize}
    \item \textbf{Permanent}: static, DNS, CDNs.
    \item \textbf{Server-initiated}: dynamic, to handle load.
    \item \textbf{Client-initiated}: caching, shared by clients.
  \end{itemize}

  \section{What do to if a data is changed}

  \begin{itemize}
    \item \textbf{Notify}: send invalidation only (few reads).
    \item \textbf{Transfer data}: full update (many reads).
    \item \textbf{Transfer operation}: send op to apply (active replication).
  \end{itemize}

  \section{Update propagation}

  \begin{itemize}
    \item \textbf{Push}: updates sent to all (low latency).
    \item \textbf{Pull}: replicas ask when needed (high write/read ratio).
    \item \textbf{Lease-based}: control frequency of polling.
  \end{itemize}

  \section{Strategies}

  \subsection{Leader-based}
  \begin{itemize}
    \item Synchronous / async / semi-sync propagation.
  \end{itemize}

  \subsection{Leaderless}
  \begin{itemize}
    \item \textbf{Read repair}: detect + fix during read.
    \item \textbf{Anti-entropy}: background reconciliation.
  \end{itemize}

  \chapter{Big data}
  \section{Case Studies}

  \subsection{Spanner}
  \begin{itemize}
    \item Paxos replication, 2PC commit, timestamp-based concurrency.
    \item TrueTime API gives precise time with bounded uncertainty $\rightarrow$
          linearizability.
    \item Read-only transactions get timestamp and read data from that timestamp; no
          locking.
  \end{itemize}

  \subsection{Calvin}
  \begin{itemize}
    \item Paxos-based replicated log.
    \item Transactions are appended to the log.
    \item No need for a commit protocol because if there is a failure thre transaction
          doesn't need to be aborted. Just commit on every node and the failed one will
          catch up by replay the log.
  \end{itemize}

  \subsection{VoltDB}
  \begin{itemize}
    \item Manual partitioning.
    \item Single-partition transactions executed locally without coordination.
  \end{itemize}

  \section{MapReduce}

  \subsection{What the platform does}
  \begin{itemize}
    \item Scheduling: assign map/reduce tasks.
    \item Data distribution: shuffle from mappers to reducers.
    \item Fault tolerance: rerun failed tasks.
  \end{itemize}

  \subsection{Architecture}
  \begin{itemize}
    \item Master assigns tasks to workers.
    \item Map phase: split input $\to$ process $\to$ local files.
    \item Reduce phase: read intermediate data $\to$ reduce $\to$ output.
  \end{itemize}

  \subsection{Data Locality}
  \begin{itemize}
    \item Minimize network usage.
    \item Map tasks scheduled near data (e.g., GFS replicas).
  \end{itemize}

  \subsection{Fault Tolerance}
  Master evaluates health of workers with heartbeat
  \begin{itemize}
    \item Worker crash: redo map/reduce tasks.
    \item Master crash: checkpoint state to recover.
  \end{itemize}

  \subsection{Stragglers}
  \begin{itemize}
    \item Slow tasks duplicated to reduce overall latency.
  \end{itemize}

  \subsection{Limitations}
  \begin{itemize}
    \item High overhead, fixed Map/Reduce steps.
    \item Each step must finish before the next starts.
  \end{itemize}

  \section{Beyond MapReduce: Dataflow Systems}

  \subsection{Motivation}
  \begin{itemize}
    \item Move from 2-step (map/reduce) to DAG of transformations.
    \item Support for iterative and streaming computations.
  \end{itemize}

  \section{Apache Spark}

  \subsection{Model}
  \begin{itemize}
    \item Arbitrary stages instead of just map/reduce.
    \item Intermediate results can be cached in memory.
  \end{itemize}

  \subsection{Micro-batch Streaming}
  \begin{itemize}
    \item Input stream split into small batches.
    \item Persistent state across batches.
  \end{itemize}

  \section{Apache Flink}

  \subsection{Model}
  \begin{itemize}
    \item All operators instantiated at job submission.
    \item Data flows continuously through TCP channels.
  \end{itemize}

  \subsection{Streaming}
  \begin{itemize}
    \item Ideal for low latency stream processing.
    \item Same architecture used for batch (stream the batch).
  \end{itemize}

  \section{Comparison: Spark vs Flink}

  \subsection{Latency}
  \begin{itemize}
    \item \textbf{Flink}: lower latency (no batching, pipelining).
  \end{itemize}

  \subsection{Throughput}
  \begin{itemize}
    \item \textbf{Spark}: better throughput (batching optimizations, compression).
  \end{itemize}

  \subsection{Load Balancing}
  \begin{itemize}
    \item \textbf{Spark}: dynamic scheduling allows balancing at runtime.
    \item \textbf{Flink}: tasks assigned statically at deployment.
  \end{itemize}

  \subsection{Elasticity}
  \begin{itemize}
    \item \textbf{Spark}: dynamically add/remove resources.
    \item \textbf{Flink}: requires snapshot + restart for reallocation.
  \end{itemize}

  \subsection{Fault Tolerance}
  \begin{itemize}
    \item \textbf{Spark}: If someone fails, redo his computation.
    \item \textbf{Flink}: checkpoint \& replay (Chandy-Lamport).
  \end{itemize}

  \chapter{Peer-to-Peer (P2P)}

  \subsection{Definition}
  \begin{itemize}
    \item Distributed systems without central control.
    \item Each node can act as client, server, and router.
  \end{itemize}

  \subsection{Motivation}
  \begin{itemize}
    \item Popular in 2000s for file sharing (e.g., Napster).
    \item Use idle resources at the edges (bandwidth, CPU, storage).
  \end{itemize}

  \subsection{Characteristics}
  \begin{itemize}
    \item No central administration.
    \item Highly dynamic: nodes can join/leave anytime.
    \item Variable node capabilities.
    \item Internet-scale; no global view.
  \end{itemize}

  \subsection{Overlay Networks}
  \begin{itemize}
    \item Logical links on top of the physical network.
    \item Peers form a virtual network for routing/search.
  \end{itemize}

  \section{Resource Retrieval}

  \subsection{Types of Queries}
  \begin{itemize}
    \item \textbf{Search}: find all items matching a query.
    \item \textbf{Lookup}: find a specific item.
  \end{itemize}

  \subsection{What is Retrieved?}
  \begin{itemize}
    \item \textbf{Data}: actual file (burdens the overlay).
    \item \textbf{Reference}: location of the file (preferred).
  \end{itemize}

  \section{Centralized Search: Napster}

  \subsection{How it Works}
  \begin{itemize}
    \item Clients contact a central server to:
          \begin{itemize}
            \item \textbf{Join}: register with server.
            \item \textbf{Publish}: send to server the list of shared I am hosting.
            \item \textbf{Search}: Ask the centralized server who has a resource.
            \item \textbf{Fetch}: In p2p get the file
          \end{itemize}
    \item File transfer happens peer-to-peer.
  \end{itemize}

  \subsection{Pros}
  \begin{itemize}
    \item Simple design.
    \item $O(1)$ search scope.
  \end{itemize}

  \subsection{Cons}
  \begin{itemize}
    \item Server maintains $O(N)$ state.
    \item Centralized search processing.
    \item Single point of failure/control.
  \end{itemize}

  \subsection{Is Napster True P2P?}
  \begin{itemize}
    \item Partially: data transfer is P2P.
    \item Lookup is centralized $\rightarrow$ not pure P2P.
  \end{itemize}

  \section{Query Flooding - Gnutella}

  \subsection{How it Works}
  \begin{itemize}
    \item No central server, fully decentralized.
    \item Join: Join the network: contact nodes that become neighbors.
    \item Search: flood query to neighbors (Hop-To-Live limit).
    \item Fetch: download directly from peer.
  \end{itemize}

  \subsection{Pros}
  \begin{itemize}
    \item Fully decentralized, no central coordination.
    \item Search supports flexible queries (text, fuzzy search).
  \end{itemize}

  \subsection{Cons}
  \begin{itemize}
    \item Network flooding: C neighbours and D hop to live causes on average $C \times D$
          messages per query.
    \item Search scope: $O(N)$, time: $O(2D)$.
    \item High churn: nodes join/leave frequently.
  \end{itemize}

  \section{Hierarchical P2P - Kazaa}

  \subsection{Architecture}
  \begin{itemize}
    \item Two-tier topology: \textbf{supernodes} and normal nodes.
    \item Normal nodes contact supernodes for search.
    \item Supernodes flood the query among themselves.
  \end{itemize}

  \subsection{Workflow}
  \begin{itemize}
    \item Join: connect to a supernode.
    \item Publish: send list of files to supernode.
    \item Search: query supernode, supernodes flood query.
    \item Fetch: direct peer-to-peer transfer, possibly parallel.
  \end{itemize}

  \subsection{Pros \& Cons}
  \begin{itemize}
    \item Pros: handles heterogeneity (bandwidth, CPU).
    \item Cons: no guarantees on search time/scope.
  \end{itemize}

  \section{Collaborative P2P - BitTorrent}

  \subsection{Basics}
  \begin{itemize}
    \item Join: contact a centralized \textbf{tracker}, get peers list.
    \item Publish: run a tracker.
    \item Search: out-of-band (e.g., Google search to find a tracker for the file).
    \item Fetch: exchange file fragments among peers.
  \end{itemize}

  \subsection{Terminology}
  \begin{itemize}
    \item \textbf{Torrent}: metadata file (file names, sizes, hashes).
    \item \textbf{Seed}: peer with complete file.
    \item \textbf{Leech}: peer downloading the file.
    \item \textbf{Swarm}: all peers (seeds + leeches).
  \end{itemize}

  \subsection{Fragment Distribution}
  \begin{itemize}
    \item File split into fixed-size fragments (e.g., 256KB).
    \item Peers use \textbf{rarest-first} to download least common pieces. This tries to
          keep the number or replicas for each fragment high.
    \item Upload of the just dowloaded fragments starts before full download completes.
    \item Files can be downloaded by every peer if there is at least a distributed copy
          of the file. Even if there are no seeds!
  \end{itemize}

  \subsection{Tit-for-Tat Strategy}
  \begin{itemize}
    \item "I upload to you if you upload to me."
    \item \textbf{Choking}: temporarily refuse to upload to slow peers.
    \item \textbf{Optimistic unchoke}: explore new connections.
  \end{itemize}

  \subsection{Pros \& Cons}
  \begin{itemize}
    \item Pros: reduces free-riding, scalable file sharing.
    \item Cons: needs central tracker to bootstrap.
  \end{itemize}

  \section{Freenet}

  \subsection{Goals}
  \begin{itemize}
    \item Anonymous publishing and retrieval.
    \item No central control; scalable and censorship-resistant.
  \end{itemize}

  \subsection{Operations}
  \begin{itemize}
    \item Join: connect to known peers; get node ID.
    \item Publish: route file contents toward the node which stores other files whose id
          is closest to file id
    \item Search: reoute query for file id using hill-climbing search with backtracking.
    \item Fetch: result is returned back; cached along path.
  \end{itemize}

  \subsection{Routing}
  \begin{itemize}
    \item Nodes store local data + neighbor hints.
    \item Forward request to "best guess" neighbor.
    \item Cache data on the way back (LRU replacement).
  \end{itemize}

  \subsection{Properties}
  \begin{itemize}
    \item Data migrates towards areas of demand.
    \item Popular data is widely cached.
    \item Anonymity via routing indirection and encryption.
    \item To avoid censorship, the id of files should not have collisions, otherwise
          censorship could be achieved by crafting a colliding file, effectively removing
          access to the original one.
    \item Encript documents, so the cachers don't know its content.
  \end{itemize}

  Pros:
  \begin{itemize}
    \item Intelligent routing makes queries relatively short
    \item Anonymity.
  \end{itemize}

  Cons:
  \begin{itemize}
    \item Anonymity features make it hard to debug and profile.
    \item Still no guarantees on query time
  \end{itemize}

  \chapter{DHT}
  Is a distributed hash table that exposes the classic abstraction
  \texttt{put(id,item)} \texttt{get(id)}. They are implemented with an overlay
  network of brokers that organize in a Ring, Tree, Hypercube, SkipList. They can
  be used for flat naming by storing the name as key and the address as value.

  \section{Chord Implementation}

  Brokers and keys are organized in a circular manner. The size of the key space
  is m (so $2^m$ keys are available). Each broker and key is assigned an m bit id
  (usually the hash of the IP address and the hash of the keys). Key k is managed
  by the first broker with an index greater than k (the successor). Brokers have
  a direct connection to the successor broker and they keep a finger table with m
  entries where entry i in the finger table of node n is the first node whose id
  is higher or equal than $n + 2^i$. When looking for a key a client connects to
  the closest broker and sends a get(key) request. The broker connects with the
  broker closest to the key using his finger table. This procedure has an O(m)
  comlexity.

  \textbf{Join}

  When a new node n wants to join the Chord, the successor, finger table and
  predecessor need to be initialized. We suppose that n knows at least another
  node n', so will make to n' m requests to initialize his fields. Then the other
  nodes in the Chord need to be updated: nodes q where n might be in position i
  in their finger table are nodes where $suc(q+2^i) = n$. The cost of updating
  them is $m^2$, because we need to find m nodes and update their table
  (constant). Then node n needs to obtain a copy of the key value pairs he
  handles by asking them to his successor.

  \textbf{Failure resilience}

  To tolerate failures we need to replicate the key value pairs in each node. We
  replicate them in K successors. Every node keeps a direct pointer to K
  successors. When an operation is issued we lazily check if the node pointed by
  the entry in the finger table is still up and if it is not we update.

  \textbf{Limitations}

  Chord implementation is rarely used because it doesn't take into account the
  physical topology of the network (a suc might be a WAN hop away). Then is not
  suitable for inexact matches. Another limitation is the creation of hotspots in
  case of popular key value pairs. This can be mitigated with caching. A possible
  solution to the first problem might be to modify the rule for the finger table
  construction by also taking into account the distance of the node and
  prioritize closer nodes.

\end{multicols}
\end{document}